{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aandrijana/Image-Colorization-Project/blob/main/final_data_preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8yGOszBFmaJ"
      },
      "source": [
        "# Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nu0ft9urF6yv"
      },
      "source": [
        "For training, the number of luminance (L) images must match the chrominance (AB) image pairs. Our dataset has 25,000 grayscale L images, two sets of 10,000 and one set of 5000 AB pairs (ab1, ab2, ab3).\n",
        "\n",
        "Due to Google Colab's memory constraints, we're initially using a subset: 10,000 grayscale images paired with ab1. This helps prevent crashes but may result in lower model accuracy compared to using the full dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ixAm751wloID"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.models import Model\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import cv2\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "37PHAPWvFxvd",
        "outputId": "74cc97ed-c484-4508-c928-a7603be0f729"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gray image shape: (10000, 224, 224)\n",
            "AB image shape: (10000, 224, 224, 2)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "l_channel = np.load(\"image_colorization_data/l/gray_scale.npy\")[:10000]\n",
        "ab = np.load(\"image_colorization_data/ab/ab/ab1.npy\")\n",
        "print(\"Gray image shape:\", l_channel.shape)\n",
        "print(\"AB image shape:\", ab.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Resize to 128x128"
      ],
      "metadata": {
        "id": "pbI7VIZPiMrS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e0YQY3W4C0Xk"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "def resize_l_ab(l_array, ab_array, target_shape=(128, 128)):\n",
        "    resized_l = []\n",
        "    resized_ab = []\n",
        "\n",
        "    for l_img, ab_img in zip(l_array, ab_array):\n",
        "        # Resizing L channel\n",
        "        l_resized = cv2.resize(l_img, target_shape, interpolation=cv2.INTER_AREA)\n",
        "\n",
        "        # Resizing A and B channels separately\n",
        "        a_resized = cv2.resize(ab_img[:, :, 0], target_shape, interpolation=cv2.INTER_AREA)\n",
        "        b_resized = cv2.resize(ab_img[:, :, 1], target_shape, interpolation=cv2.INTER_AREA)\n",
        "        ab_resized = np.stack((a_resized, b_resized), axis=-1)\n",
        "\n",
        "        resized_l.append(l_resized)\n",
        "        resized_ab.append(ab_resized)\n",
        "\n",
        "    return np.array(resized_l), np.array(resized_ab)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U_ksJKw7VCNh",
        "outputId": "eb6347e3-8ac1-4688-90eb-7bb85b06cb43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gray image shape: (10000, 128, 128)\n",
            "AB image shape: (10000, 128, 128, 2)\n"
          ]
        }
      ],
      "source": [
        "l_channel, ab= resize_l_ab(l_channel, ab)\n",
        "print(\"Gray image shape:\", l_channel.shape)\n",
        "print(\"AB image shape:\", ab.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RL_7Ht4ivK5"
      },
      "source": [
        "We resized the input images from 224×224 pixels to 128×128 (recommended size in [pix2pix paper](https://arxiv.org/pdf/1611.07004v3) ) pixels. This downscaling significantly reduces the memory footprint, which is crucial for preventing out-of-memory errors and enabling more efficient training, especially on systems with limited RAM."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOC29npZRp8T"
      },
      "source": [
        "### Filter Outliers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ZChNRMkwESS"
      },
      "outputs": [],
      "source": [
        "# Remove over/under-exposed images (L channel)\n",
        "mean_brightness = np.mean(l_channel, axis=(1, 2))\n",
        "# Tighten the brightness range based on your distribution\n",
        "valid_indices = np.where((mean_brightness >= 50) & (mean_brightness <= 170))[0]\n",
        "l_filtered = l_channel[valid_indices]\n",
        "ab_filtered = ab[valid_indices]\n",
        "\n",
        "# Remove low-colorfulness images (AB channels)\n",
        "colorfulness = np.std(ab, axis=(1, 2, 3))\n",
        "# Increase threshold to remove bland/grayscale images\n",
        "valid_indices = np.where(colorfulness > 10)[0]\n",
        "l_filtered = l_channel[valid_indices]\n",
        "ab_filtered = ab[valid_indices]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Leveraging insights from the Data exploration phase, specifically the **colorfulness and brightness distribution plots**, we performed **outlier filtering**. This process involved identifying and removing images that exhibited extreme or anomalous values in these characteristics, which can negatively impact model training and performance. This step ensures a more robust and representative training dataset."
      ],
      "metadata": {
        "id": "MloRjGvPhRbI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Splitting the data into training, validation and test sets"
      ],
      "metadata": {
        "id": "fWgZ7MGdiWQy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#dimension is 3\n",
        "if l_filtered.ndim == 3:\n",
        "    l_filtered = l_filtered[..., np.newaxis]"
      ],
      "metadata": {
        "id": "4zzyHkyky3Hc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZqxaFrXxS0GZ"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split #we have no classes, so we'll just go with regular seed 42\n",
        "l_train, l_test, ab_train, ab_test = train_test_split(l_filtered, ab_filtered, test_size=0.1, random_state=42)\n",
        "l_train, l_val, ab_train, ab_val = train_test_split(l_train, ab_train, test_size=0.1, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Normalization to [-1,1]"
      ],
      "metadata": {
        "id": "C-Ska0VDig3Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To prepare the LAB image data for our pix2pix model, we perform a crucial normalization step. The pix2pix architecture, like many deep learning models, generally performs best when input data is scaled to a specific range, typically [-1, 1]."
      ],
      "metadata": {
        "id": "MK6h4E3eiFta"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"L range:\", np.min(l_train), \"to\", np.max(l_train))\n",
        "print(\"A range:\", np.min(ab_train[:,:,:,0]), \"to\", np.max(ab_train[:,:,:,0]))\n",
        "print(\"B range:\", np.min(ab_train[:,:,:,1]), \"to\", np.max(ab_train[:,:,:,1]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uAofqIferHTv",
        "outputId": "a315fcd0-2257-405c-b733-251b92069a48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "L range: 0 to 255\n",
            "A range: 42 to 226\n",
            "B range: 20 to 223\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iGMm7bK5gVY2"
      },
      "outputs": [],
      "source": [
        "L_IN_MIN, L_IN_MAX = 0.0, 255.0\n",
        "A_IN_MIN, A_IN_MAX = 42.0, 226.0\n",
        "B_IN_MIN, B_IN_MAX = 20.0, 223.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S7qMjQh1SN3K"
      },
      "outputs": [],
      "source": [
        "def normalize_data(l_channel, ab_channels):\n",
        "    \"\"\"\n",
        "    Casts data to float32 and normalizes from the CUSTOM source ranges to [-1, 1].\n",
        "    \"\"\"\n",
        "    # Cast to float32 first\n",
        "    #Neural networks perform calculations with floating-point numbers, so this step is essential.\n",
        "    l_channel = tf.cast(l_channel, tf.float32)\n",
        "    ab_channels = tf.cast(ab_channels, tf.float32)\n",
        "\n",
        "    # Separating A and B channels from the (h, w, 2) tensor\n",
        "    # We use slicing to keep the final dimension, which makes concatenation easy\n",
        "    a_channel = ab_channels[..., 0:1]\n",
        "    b_channel = ab_channels[..., 1:2]\n",
        "\n",
        "    # Generic formula for mapping [min, max] to [-1, 1] is: 2 * (x - min) / (max - min) - 1\n",
        "    l_norm = 2 * (l_channel - L_IN_MIN) / (L_IN_MAX - L_IN_MIN) - 1\n",
        "    a_norm = 2 * (a_channel - A_IN_MIN) / (A_IN_MAX - A_IN_MIN) - 1\n",
        "    b_norm = 2 * (b_channel - B_IN_MIN) / (B_IN_MAX - B_IN_MIN) - 1\n",
        "\n",
        "    # Re-combining the normalized A and B channels\n",
        "    ab_norm = tf.concat([a_norm, b_norm], axis=-1)\n",
        "\n",
        "    return l_norm, ab_norm\n",
        "\n",
        "l_train, ab_train =normalize_data(l_train, ab_train)\n",
        "l_test, ab_test= normalize_data(l_test, ab_test)\n",
        "l_val, ab_val=normalize_data(l_val, ab_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The normalization formula used is <br> $$\n",
        "2 \\cdot \\frac{x - \\min}{\\max - \\min} - 1\n",
        "$$ <br>\n",
        "which effectively maps the custom input range to the desired [-1, 1] output range. This transformation is applied uniformly to our training, testing, and validation datasets, making them suitable for the model's activation functions and improving training stability."
      ],
      "metadata": {
        "id": "K3TWzCABixL9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Augmentation"
      ],
      "metadata": {
        "id": "xUz6gyJUkUwY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To improve generalization and prevent overfitting, we implement data augmentation via random horizontal flipping. There's a 50% chance of applying a horizontal flip to both the luminance (L) channel and its corresponding chrominance (AB) channels simultaneously. This ensures the spatial relationship between the grayscale input and its color information remains consistent, effectively expanding our training data."
      ],
      "metadata": {
        "id": "IGvApPCckzw0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WrRMVhZIuusQ"
      },
      "outputs": [],
      "source": [
        "def augment(l_channel, ab_channels):\n",
        "    \"\"\"Applies identical random horizontal flip to both L and AB channels.\"\"\"\n",
        "    if tf.random.uniform(()) > 0.5:\n",
        "        l_channel = tf.image.flip_left_right(l_channel)\n",
        "        ab_channels = tf.image.flip_left_right(ab_channels)\n",
        "    return l_channel, ab_channels"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TensorFlow Dataset"
      ],
      "metadata": {
        "id": "gP8QLmOZKtV3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DRIVE_BASE_PATH = \"/content/drive/MyDrive/ImageColorization\"\n",
        "MODELS_DIR = os.path.join(DRIVE_BASE_PATH, \"saved_models\")\n",
        "PROGRESS_DIR = os.path.join(DRIVE_BASE_PATH, \"training_progress\")\n",
        "\n",
        "os.makedirs(MODELS_DIR, exist_ok=True)\n",
        "os.makedirs(PROGRESS_DIR, exist_ok=True)\n",
        "\n",
        "def create_tf_dataset(l_data, ab_data, batch_size=32, shuffle=True,augment_data=False):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((l_data, ab_data))\n",
        "    if shuffle:\n",
        "        dataset = dataset.shuffle(buffer_size=len(l_data))\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    # Apply augmentation AFTER batching and ONLY if specified\n",
        "    if augment_data:\n",
        "        dataset = dataset.map(augment, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    # Prefetch for performance\n",
        "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
        "    return dataset\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "train_dataset = create_tf_dataset(l_train, ab_train, BATCH_SIZE)\n",
        "val_dataset = create_tf_dataset(l_val, ab_val, BATCH_SIZE, shuffle=False)\n",
        "test_dataset = create_tf_dataset(l_test, ab_test, BATCH_SIZE, shuffle=False)"
      ],
      "metadata": {
        "id": "5CNAjWHwKxLj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Shuffling: For the training set, we shuffle the data before batching to ensure\n",
        "diverse batches and improve model generalization. Shuffling is omitted for validation and test sets.\n",
        "* Batching: Data is grouped into batches of 32, optimizing GPU utilization and stabilizing gradient updates during training.\n",
        "* Augmentation\n",
        "* Prefetching: allows the next batch of data to be prepared in the background while the current batch is being processed, maximizing throughput and preventing idle time."
      ],
      "metadata": {
        "id": "AVh67TQ5K2LV"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}